{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__grader__=\"Emily Hua\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import vincenty\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "load the json submission file, store as list @submission\n",
    "'''\n",
    "import json\n",
    "\n",
    "submission = []\n",
    "with open('submission') as f:\n",
    "    for line in f:\n",
    "        submission.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node1  node2\n",
       "0      0      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "@nodes stores the edges\n",
    "'''\n",
    "nodes = pd.read_csv(\"../usersGPS_larg_comp_index.txt\", header = None, sep=' ')\n",
    "nodes.columns = ['node1', 'node2']\n",
    "nodes[:1]#preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@G stores the undirected graph\n",
    "'''\n",
    "import networkx as nx\n",
    "G = nx.from_pandas_dataframe(nodes, 'node1', 'node2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "validate the path\n",
    "'''\n",
    "def is_valid (path, G):\n",
    "    for index in range(len(path) - 1): \n",
    "        if (not(path[index] in G.neighbors(path[index + 1]))): #no an edge\n",
    "            return False\n",
    "    return True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "uni transformation\n",
    "@uni_map\n",
    "generate a list of uni\n",
    "@uni_list\n",
    "'''\n",
    "from collections import defaultdict\n",
    "uni_map = defaultdict(str)\n",
    "uni_list = []\n",
    "with open(\"new_uni.csv\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n').split(\",\")\n",
    "        uni_map[line[1]] = line[0]   \n",
    "        uni_list.append(line[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "return the length of the path\n",
    "'''\n",
    "def find_length(path):\n",
    "    return len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''load the checkin data'''\n",
    "df_checkin = pd.read_csv(\"../checkin_usersGPS_largcomp_index.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>latitude</th>\n",
       "      <th>logitude</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6200</td>\n",
       "      <td>40.82942</td>\n",
       "      <td>-73.920529</td>\n",
       "      <td>2013-08-31 20:45:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6200</td>\n",
       "      <td>40.82942</td>\n",
       "      <td>-73.920529</td>\n",
       "      <td>2013-08-31 20:42:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node  latitude   logitude                date\n",
       "0  6200  40.82942 -73.920529 2013-08-31 20:45:49\n",
       "1  6200  40.82942 -73.920529 2013-08-31 20:42:34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''process the checkin data '''\n",
    "df_checkin.columns = ['node', 'latitude', 'logitude','epoch']\n",
    "df_checkin['node'] = df_checkin['node'].astype(int)\n",
    "df_checkin['date'] = pd.to_datetime(df_checkin['epoch'],unit='s')\n",
    "del df_checkin['epoch']\n",
    "#preview first 2 entries\n",
    "df_checkin[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''to keep only the last checkin'''\n",
    "idx = df_checkin.groupby(['node'])['date'].transform(max) == df_checkin['date']\n",
    "#print (idx)\n",
    "df_last_checkin = df_checkin[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-74.097142738000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_last_checkin['latitude'][df_last_checkin['node'] == 0].values[0]\n",
    "df_last_checkin['logitude'][df_last_checkin['node'] == 0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "return the distance of the path\n",
    "@get_distance can either return eculidean or geo distance\n",
    "'''\n",
    "def get_distance( path, df_checkin):\n",
    "    dist_euclidean_sum = 0\n",
    "    geo_dist_sum = 0\n",
    "    for index in range(len(path) - 1 ):\n",
    "        distances1 = []\n",
    "        distances2 = []\n",
    "        node1 = path[index]\n",
    "        node2 = path[index+1]\n",
    "        latitude1 = df_last_checkin['latitude'][df_last_checkin['node'] == node1].values[0]\n",
    "        logitude1 = df_last_checkin['logitude'][df_last_checkin['node'] == node1].values[0]\n",
    "        latitude2 = df_last_checkin['latitude'][df_last_checkin['node'] == node2].values[0]\n",
    "        logitude2 = df_last_checkin['logitude'][df_last_checkin['node'] == node2].values[0]\n",
    "        distances1.append(latitude1)\n",
    "        distances1.append(logitude1)\n",
    "        distances2.append(latitude2)\n",
    "        distances2.append(logitude2)        \n",
    "        distances1 = np.array([distances1])\n",
    "        distances2 = np.array([distances2])\n",
    "\n",
    "        eulidean_dist = np.linalg.norm(distances1 - distances2)\n",
    "        dist_euclidean_sum += eulidean_dist       \n",
    "        geo_dist = vincenty(distances1, distances2).miles\n",
    "        geo_dist_sum += geo_dist\n",
    "    return dist_euclidean_sum, geo_dist_sum\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real uni:  wh2353\n",
      "uni:  flag\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create nested list to fromat submission the following way:\n",
    "@table\n",
    "testcase, uni, nOfQueries, path, isvalid, euclidean_dist, geo_dist\n",
    "for testcase1-20\n",
    "'''\n",
    "table = []\n",
    "counter = 0\n",
    "for entry in submission:\n",
    "    uni = uni_map[entry['uni']['s']]\n",
    "    nOfQueries = entry['nOfQueries']['n']\n",
    "    testcaseID = entry['testcaseID']['s']\n",
    "    if (entry['path']['s'] == \"null\"):\n",
    "        isvalid = -1\n",
    "        path_length = -1\n",
    "        euclidean_dist = -1\n",
    "        geo_dist = -1\n",
    "        \n",
    "    else:\n",
    "        path = [int(node) for node in entry['path']['s'].strip(\"\\'\").split(',')]\n",
    "        isvalid = is_valid(path, G)\n",
    "        path_length = find_length(path)\n",
    "        euclidean_dist, geo_dist = get_distance(path, df_checkin)\n",
    "    table.append([testcaseID, uni, nOfQueries, path, isvalid, path_length, euclidean_dist, geo_dist])\n",
    "       \n",
    "    if (counter == 800):\n",
    "        uni = uni_map[entry['uni']['s']]\n",
    "        print (\"real uni: \", uni)\n",
    "        print (\"uni: \", entry['uni']['s'])\n",
    "        nOfQueries = entry['nOfQueries']['n']\n",
    "\n",
    "    counter += 1\n",
    "    #print (\"counter: \", counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "convert nested list to panda dataframe\n",
    "'''\n",
    "df_table = pd.DataFrame(table)\n",
    "df_table.columns = ['testcaseID', 'uni', 'nOfQueries', 'path', 'is_valid', 'path_length', 'euclidean_dist', 'geo_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_on:  nOfQueries\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate total score dataframe based on nOfQueries manually\n",
    "\n",
    "'''\n",
    "for testcaseID in range(13, 14):\n",
    "    table_by_testcase = df_table[df_table['testcaseID'] == str(testcaseID)]\n",
    "    table_ready = process_table(table_by_testcase, 'nOfQueries')\n",
    "    query_ranks = gen_rank(table_ready, 'nOfQueries')\n",
    "    query_performance = gen_performance_score(query_ranks)\n",
    "    query_total_score = gen_total_score(query_performance)\n",
    "    query_total_score['testcaseID'] = testcaseID\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testcaseID:  1\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  2\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  3\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  4\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  5\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  6\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  7\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  8\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  9\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  10\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  11\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  12\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  13\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  14\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  15\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  16\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  17\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  18\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  19\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  20\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  21\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  22\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  23\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  24\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  25\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  26\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  27\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  28\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  29\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  30\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  31\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  32\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  33\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  34\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  35\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  36\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  37\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  38\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  39\n",
      "base_on:  nOfQueries\n",
      "testcaseID:  40\n",
      "base_on:  nOfQueries\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate total score dataframe based on nOfQueries\n",
    "@query_score\n",
    "'''\n",
    "\n",
    "from collections import defaultdict\n",
    "query_score = defaultdict(float)\n",
    "for testcaseID in range(1, 41):   \n",
    "    print (\"testcaseID: \", testcaseID)\n",
    "    table_by_testcase = df_table[df_table['testcaseID'] == str(testcaseID)]\n",
    "    table_ready = process_table(table_by_testcase, 'nOfQueries')\n",
    "    query_ranks = gen_rank(table_ready, 'nOfQueries')\n",
    "    query_performance = gen_performance_score(query_ranks)\n",
    "    query_total_score = gen_total_score(query_performance)\n",
    "    query_total_score['testcaseID'] = testcaseID\n",
    "    if (testcaseID == 1):\n",
    "        giant_query = query_total_score\n",
    "    else:\n",
    "        giant_query = [giant_query, query_total_score]\n",
    "        giant_query = pd.concat(giant_query)\n",
    "    for index, row in query_total_score.iterrows():\n",
    "        key = frozenset({row['testcaseID'], row['uni']})\n",
    "        query_score[key] = row['total_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giant_query.columns = ['uni','query_rank', 'query_performance_score', 'query_lowest_rank', 'query_total_score', 'testcaseID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giant_query.query_rank[giant_query['query_rank'] == 9223372036854775807 ] = 'invalid path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giant_len.columns = ['uni', 'length_rank', 'length_performance_score', 'length_lowest_rank', 'length_total_score', 'testcaseID']\n",
    "giant_len.length_rank[giant_len['length_rank'] == 9223372036854775807] = 'invalid path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giant_geo.columns = ['uni', 'geo_rank', 'geo_performance_score', 'geo_lowest_rank', 'geo_total_score', 'testcaseID']\n",
    "giant_geo.geo_rank[giant_geo['geo_rank'] == 9223372036854775807] = 'invalid path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giant_eucli.columns = ['uni', 'eucli_rank', 'eucli_performance_score', 'eucli_lowest_rank', 'eucli_total_score', 'testcaseID']\n",
    "giant_eucli.eucli_rank[giant_eucli['eucli_rank'] == 9223372036854775807] = 'invalid path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_task1 = pd.merge(giant_query,giant_len, on=['uni', 'testcaseID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_task2_eucli = pd.merge(giant_query,giant_eucli, on=['uni', 'testcaseID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_task2 = pd.merge(merged_task2_eucli, giant_geo, on = ['uni', 'testcaseID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_test = pd.concat([merged_task1, merged_task2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for uni in uni_list:\n",
    "    path = './grades/'\n",
    "    name = uni\n",
    "    task = '_task1'\n",
    "    filename = path + name + task + '.csv'\n",
    "    merged_task1[merged_task1.uni == uni].to_csv(filename, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testcaseID:  1\n",
      "base_on:  path_length\n",
      "testcaseID:  2\n",
      "base_on:  path_length\n",
      "testcaseID:  3\n",
      "base_on:  path_length\n",
      "testcaseID:  4\n",
      "base_on:  path_length\n",
      "testcaseID:  5\n",
      "base_on:  path_length\n",
      "testcaseID:  6\n",
      "base_on:  path_length\n",
      "testcaseID:  7\n",
      "base_on:  path_length\n",
      "testcaseID:  8\n",
      "base_on:  path_length\n",
      "testcaseID:  9\n",
      "base_on:  path_length\n",
      "testcaseID:  10\n",
      "base_on:  path_length\n",
      "testcaseID:  11\n",
      "base_on:  path_length\n",
      "testcaseID:  12\n",
      "base_on:  path_length\n",
      "testcaseID:  13\n",
      "base_on:  path_length\n",
      "testcaseID:  14\n",
      "base_on:  path_length\n",
      "testcaseID:  15\n",
      "base_on:  path_length\n",
      "testcaseID:  16\n",
      "base_on:  path_length\n",
      "testcaseID:  17\n",
      "base_on:  path_length\n",
      "testcaseID:  18\n",
      "base_on:  path_length\n",
      "testcaseID:  19\n",
      "base_on:  path_length\n",
      "testcaseID:  20\n",
      "base_on:  path_length\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate total score dataframe based on path length\n",
    "@query_score\n",
    "'''\n",
    "from collections import defaultdict\n",
    "length_score = defaultdict(float)\n",
    "for testcaseID in range(1, 21):   \n",
    "    print (\"testcaseID: \", testcaseID)\n",
    "    table_by_testcase = df_table[df_table['testcaseID'] == str(testcaseID)]\n",
    "    table_ready = process_table(table_by_testcase, 'path_length')\n",
    "    length_ranks = gen_rank(table_ready, 'path_length')\n",
    "    length_performance = gen_performance_score(length_ranks)\n",
    "    length_total_score = gen_total_score(length_performance)\n",
    "    length_total_score['testcaseID'] = testcaseID\n",
    "    if (testcaseID == 1):\n",
    "        giant_len = length_total_score\n",
    "    else:\n",
    "        giant_len = [giant_len, length_total_score]\n",
    "        giant_len = pd.concat(giant_len)\n",
    "    for index, row in length_total_score.iterrows():\n",
    "        key = frozenset({row['testcaseID'], row['uni']})\n",
    "        length_score[key] = row['total_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testcaseID:  21\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  22\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  23\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  24\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  25\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  26\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  27\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  28\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  29\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  30\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  31\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  32\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  33\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  34\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  35\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  36\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  37\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  38\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  39\n",
      "base_on:  euclidean_dist\n",
      "testcaseID:  40\n",
      "base_on:  euclidean_dist\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate total score dataframe based on euclidian dist\n",
    "@query_score\n",
    "'''\n",
    "from collections import defaultdict\n",
    "eucli_score = defaultdict(float)\n",
    "for testcaseID in range(21, 41):   \n",
    "    print (\"testcaseID: \", testcaseID)\n",
    "    table_by_testcase = df_table[df_table['testcaseID'] == str(testcaseID)]\n",
    "    table_ready = process_table(table_by_testcase, 'euclidean_dist')\n",
    "    eucli_ranks = gen_rank(table_ready, 'euclidean_dist')\n",
    "    eucli_performance = gen_performance_score(eucli_ranks)\n",
    "    eucli_total_score = gen_total_score(eucli_performance)\n",
    "    eucli_total_score['testcaseID'] = testcaseID\n",
    "    if (testcaseID == 21):\n",
    "        giant_eucli = eucli_total_score\n",
    "    else:\n",
    "        giant_eucli = [giant_eucli, eucli_total_score]\n",
    "        giant_eucli = pd.concat(giant_eucli)\n",
    "    for index, row in eucli_total_score.iterrows():\n",
    "        key = frozenset({row['testcaseID'], row['uni']})\n",
    "        eucli_score[key] = row['total_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testcaseID:  21\n",
      "base_on:  geo_dist\n",
      "testcaseID:  22\n",
      "base_on:  geo_dist\n",
      "testcaseID:  23\n",
      "base_on:  geo_dist\n",
      "testcaseID:  24\n",
      "base_on:  geo_dist\n",
      "testcaseID:  25\n",
      "base_on:  geo_dist\n",
      "testcaseID:  26\n",
      "base_on:  geo_dist\n",
      "testcaseID:  27\n",
      "base_on:  geo_dist\n",
      "testcaseID:  28\n",
      "base_on:  geo_dist\n",
      "testcaseID:  29\n",
      "base_on:  geo_dist\n",
      "testcaseID:  30\n",
      "base_on:  geo_dist\n",
      "testcaseID:  31\n",
      "base_on:  geo_dist\n",
      "testcaseID:  32\n",
      "base_on:  geo_dist\n",
      "testcaseID:  33\n",
      "base_on:  geo_dist\n",
      "testcaseID:  34\n",
      "base_on:  geo_dist\n",
      "testcaseID:  35\n",
      "base_on:  geo_dist\n",
      "testcaseID:  36\n",
      "base_on:  geo_dist\n",
      "testcaseID:  37\n",
      "base_on:  geo_dist\n",
      "testcaseID:  38\n",
      "base_on:  geo_dist\n",
      "testcaseID:  39\n",
      "base_on:  geo_dist\n",
      "testcaseID:  40\n",
      "base_on:  geo_dist\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate total score dataframe based on geo dist\n",
    "@query_score\n",
    "'''\n",
    "from collections import defaultdict\n",
    "geo_score = defaultdict(float)\n",
    "for testcaseID in range(21, 41):   \n",
    "    print (\"testcaseID: \", testcaseID)\n",
    "    table_by_testcase = df_table[df_table['testcaseID'] == str(testcaseID)]\n",
    "    table_ready = process_table(table_by_testcase, 'geo_dist')\n",
    "    geo_ranks = gen_rank(table_ready, 'geo_dist')\n",
    "    geo_performance = gen_performance_score(geo_ranks)\n",
    "    geo_total_score = gen_total_score(geo_performance)\n",
    "    geo_total_score['testcaseID'] = testcaseID\n",
    "    if (testcaseID == 21):\n",
    "        giant_geo = geo_total_score\n",
    "    else:\n",
    "        giant_geo = [giant_geo,geo_total_score]\n",
    "        giant_geo = pd.concat(giant_geo)\n",
    "    for index, row in geo_total_score.iterrows():\n",
    "        key = frozenset({row['testcaseID'], row['uni']})\n",
    "        geo_score[key] = row['total_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@final_score_map stores scores for task case 1-20\n",
    "'''\n",
    "final_score_map = defaultdict(float)\n",
    "for testcaseID in range(1,21):\n",
    "    for uni in uni_list:\n",
    "        key = frozenset ({testcaseID, uni})\n",
    "        final_score = query_score[key]*0.4 + length_score[key]*0.6\n",
    "        #print (\"key: \", key)\n",
    "        #print (\"length_score: \", length_score[key])\n",
    "        #print (\"query_score: \", query_score[key])\n",
    "        final_score_map[key] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@final_score_map adding scores for task case 21-40\n",
    "'''\n",
    "for testcaseID in range(21,41):\n",
    "    for uni in uni_list:\n",
    "        key = frozenset ({testcaseID, uni})\n",
    "        final_score = query_score[key]*0.4 + max(geo_score[key], eucli_score[key])*0.6\n",
    "        #print (\"key: \", key)\n",
    "        #print (\"dist_score: \", max(geo_score[key], eucli_score[key]))\n",
    "        #print (\"query_score: \", query_score[key])\n",
    "        final_score_map[key] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  1 2.442857142857143\n",
      "id:  2 2.4857142857142858\n",
      "id:  3 2.3666666666666667\n",
      "id:  4 2.1799999999999997\n",
      "id:  5 2.065384615384615\n",
      "id:  6 2.3041666666666663\n",
      "id:  7 2.256666666666667\n",
      "id:  8 0.0\n",
      "id:  9 0.0\n",
      "id:  10 2.194871794871795\n",
      "id:  11 0.0\n",
      "id:  12 2.1952380952380954\n",
      "id:  13 0.0\n",
      "id:  14 0.0\n",
      "id:  15 2.2857142857142856\n",
      "id:  16 0.0\n",
      "id:  17 0.0\n",
      "id:  18 0.0\n",
      "id:  19 0.0\n",
      "id:  20 0.0\n",
      "id:  21 0.0\n",
      "id:  22 0.0\n",
      "id:  23 0.0\n",
      "id:  24 0.0\n",
      "id:  25 0.0\n",
      "id:  26 0.0\n",
      "id:  27 0.0\n",
      "id:  28 0.0\n",
      "id:  29 0.0\n",
      "id:  30 0.0\n",
      "id:  31 0.0\n",
      "id:  32 0.0\n",
      "id:  33 0.0\n",
      "id:  34 0.0\n",
      "id:  35 0.0\n",
      "id:  36 0.0\n",
      "id:  37 0.0\n",
      "id:  38 0.0\n",
      "id:  39 0.0\n",
      "id:  40 0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sum over all testcase for each students\n",
    "@final_grade\n",
    "'''\n",
    "final_grade = defaultdict(float)\n",
    "for testcaseID in range(1,41):\n",
    "    for uni in uni_list:\n",
    "        key = frozenset ({testcaseID, uni})\n",
    "        if (uni == 'pwh2126'):\n",
    "            print (\"id: \", testcaseID, final_score_map[key])\n",
    "        final_grade[uni] += final_score_map[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "process table_by_testcase to overwrite nOfQueries for uni with invalid path,\n",
    "in order to simulate the effect of dropping them outof the ranking\n",
    "'''\n",
    "def process_table (dataframe, based_on):\n",
    "    if based_on == 'path_length': \n",
    "        table_by_testcase.path_length[table_by_testcase.is_valid != True] = -1 \n",
    "    elif based_on == 'nOfQueries':\n",
    "        table_by_testcase.nOfQueries[table_by_testcase.is_valid != True] = -1\n",
    "    elif based_on == 'euclidean_dist':\n",
    "        table_by_testcase.euclidean_dist[table_by_testcase.is_valid != True] = -1\n",
    "    elif based_on == 'geo_dist':\n",
    "        table_by_testcase.geo_dist[table_by_testcase.is_valid != True] = -1 \n",
    "    table_ready = table_by_testcase[[\"uni\",\"nOfQueries\",\"path_length\", \"euclidean_dist\", \"geo_dist\"]]\n",
    "    return table_ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@gen_rank, given a table frame generate ranking of uni\n",
    "'''\n",
    "def gen_rank (dataframe, based_on):\n",
    "    print (\"base_on: \", based_on)\n",
    "    ranks = []\n",
    "    prev = sys.maxsize - 2\n",
    "    #print (\"min: \", prev)\n",
    "    rank = 1\n",
    "    dataframe[based_on] = dataframe[based_on].astype(float)\n",
    "    dataframe = dataframe.sort_values(by = [based_on])\n",
    "    for index, row in dataframe.iterrows():\n",
    "\n",
    "\n",
    "        if (row[str(based_on)] == -1): #treat invalid entry as sys.maxsize\n",
    "\n",
    "            ranks.append([row['uni'], sys.maxsize])\n",
    "        else:\n",
    "            if (row[str(based_on)] <= prev):\n",
    "            \n",
    "                ranks.append([row['uni'], rank])\n",
    "            else:\n",
    "            \n",
    "                rank += 1\n",
    "                ranks.append([row['uni'], rank])\n",
    "            prev = row[str(based_on)]\n",
    "            \n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@gen_performance_score: based on ranking \n",
    "'''\n",
    "def gen_performance_score(ranks):\n",
    "    df_ranks = pd.DataFrame(ranks, columns = ['uni', 'rank'])\n",
    "    min_rank = df_ranks['rank'][df_ranks['rank'] != sys.maxsize].min() #filter out invalid path records\n",
    "    max_rank = df_ranks['rank'][df_ranks['rank'] != sys.maxsize].max()\n",
    "    full_score = 0.5\n",
    "    df_ranks['performance_score'] = df_ranks.apply(lambda row: ((max_rank - row['rank'] + 1)/max_rank)*full_score, axis=1)\n",
    "    df_ranks.performance_score[df_ranks.performance_score < 0] = 0\n",
    "    df_ranks['max_rank'] = max_rank\n",
    "    return df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@gen_total_score: add 2 to performance score if there is a valid path\n",
    "'''\n",
    "def gen_total_score(dataframe):\n",
    "    dataframe['total_score'] = dataframe['performance_score'] + 2\n",
    "    dataframe.total_score[dataframe.total_score == 2] = 0\n",
    "    return dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
